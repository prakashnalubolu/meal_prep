from __future__ import annotations
import os, re, time
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import create_react_agent, AgentExecutor
from langchain_core.prompts import PromptTemplate

from tools.manager_tools import call_pantry, call_cuisine, missing_ingredients, memory as slot_memory
from langchain.memory import ConversationSummaryBufferMemory


load_dotenv()

# ── 1  LLM ──────────────────────────────────────────────────────────────────
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0.0,
    google_api_key=os.getenv("GEMINI_API_KEY"),
)

# ── 2  Tools exposed to Manager ────────────────────────────────────────────
TOOLS      = [call_pantry, call_cuisine, missing_ingredients]
TOOL_NAMES = ",".join(t.name for t in TOOLS)


# ── 3 · Chat-history memory (≤ 2 000 tokens) ───────────────────────────────
chat_memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=2000,
    return_messages  = True,
    memory_key="chat_history",
    human_prefix     = "user",
    ai_prefix        = "assistant",
)

# ── 4 · Auto-refresh pantry cache (unchanged) ──────────────────────────────
MAX_INV_AGE_SEC = 60
def ensure_fresh_inventory():
    ts = slot_memory.memories.get("inv_timestamp")
    if ts is None or (time.time() - ts) > MAX_INV_AGE_SEC:
        call_pantry("list pantry")        # refreshes cache in slot_memory

# ── 5  Prompt with routing rules ───────────────────────────────────────────
template = """
You are **MealPrepManager**, the orchestrator for a kitchen assistant.

You control two specialist agents exposed as tools:
{tools}

Routing rules:
1.  **Pantry-only query**  
    If the user's request is purely about pantry inventory  
    (add / remove / update / list) →  
    **Action → call_pantry** with their exact message.

2.  **Cuisine-only query**  
    If the request is purely about recipes, cuisines, cooking instructions, etc. -> **Action -> call_cuisine** with their exact message.

3.  **"What can I cook with what's in my pantry?" (cross-domain)**  
    3-a. **Action → call_pantry("list pantry")** → Observation (inventory lines)  
    3-b. Extract ingredient names (everything before the first “(” on each line).  
    3-c. **Action → call_cuisine** with a message like:  
        “Given these items <list>, please run `find_recipes_by_items`  
        with `cuisine=<if specified>` and `k=5`.”

4.  **Ingredient-gap query** — user asks   
    “what else do I need”, “what ingredients am I missing”,  
    “check pantry and update”, etc.  
    **Action → missing_ingredients("<dish name>")**  
    Append its natural-language sentence to your Final Answer.  
    *Never* ask the user to list their items again.

5. When you call missing_ingredients, pass *only* the dish name string, e.g.
   Action: missing_ingredients
   Action Input: "egg fried rice"
   Never wrap it like a python call.
6.  If the user says “the first / second / third dish” and recent_dishes
    contains enough items, replace it with that dish name automatically.

7. After completing all necessary calls, end with exactly one Final Answer: <your concise reply>

8. After listing recipes, compare the number listed to the user's requested number (e.g. 5).  
   If you listed fewer, append a short, polite explanation that more results would require extra pantry ingredients.
   Example: With the ingredients you have, I can only find these 3 recipe options.(3 is generated by the agent, not a fixed number, 3 as in number of dishes you give in the ouput)


Thought: …
Action: <{tool_names}>
Action Input: …
Observation: …
… (repeat) …
Thought: I now know the final answer
Final Answer: …

Begin!

{chat_history}

Question: {input}
{agent_scratchpad}
"""

PROMPT = PromptTemplate(
    template       = template,
    input_variables= ["input", "agent_scratchpad", "tools", "tool_names", "chat_history"],
)

# ── 6  Build ReAct manager agent ───────────────────────────────────────────
react_agent = create_react_agent(
    llm    = llm,
    tools  = TOOLS,
    prompt = PROMPT.partial(
        tools      = "\n".join(f"- {t.name}" for t in TOOLS),
        tool_names = TOOL_NAMES,
    ),
)

manager_agent = AgentExecutor(
    agent                 = react_agent,
    tools                 = TOOLS,
    max_iterations        = 35,
    memory = chat_memory,
    handle_parsing_errors = True,
    verbose               = True,
)
# Spoken ordinals → list index (0-based)
ORDINAL_MAP = {
    "first": 0, "1st": 0,"second": 1, "2nd": 1, "third": 2, "3rd": 2,"fourth": 3, "4th": 3,
    "fifth": 4, "5th": 4,"sixth": 5, "6th": 5,"seventh": 6, "7th": 6,"eighth": 7, "8th": 7,
    "ninth": 8, "9th": 8,"tenth": 9, "10th": 9,}

ORDINAL_TARGETS = r"(dish|one|recipe|option)" 
PRONOUNS = ["this dish", "that dish", "current dish", "it"]

def _replace_ordinals_and_pronouns(msg: str) -> str:
    """
    •  Replaces “the first / second / … dish” with the recipe name from
       memory['recent_dishes'].
    •  Replaces “this dish / that dish / it / current dish” with
       memory['current_dish'].

    Returns the transformed string (original unchanged if no match).
    """
    text = msg
    # ---- ordinal replacements ---------------------------------------------
    if "recent_dishes" in slot_memory.memories:
        for word, idx in ORDINAL_MAP.items():
            pattern = rf"\bthe\s+{word}\s+{ORDINAL_TARGETS}\b"
            if re.search(pattern, text, flags=re.I):
                try:
                    dish = slot_memory.memories["recent_dishes"][idx]
                    text = re.sub(pattern, dish, text, flags=re.I)
                except IndexError:
                    pass  # user asked for 4th but we only have 3 → leave as-is

    # ---- pronoun replacements ---------------------------------------------
    cd = slot_memory.memories.get("current_dish")
    if cd:
        for p in PRONOUNS:
            text = re.sub(rf"\b{p}\b", cd, text, flags=re.I)

    return text

#  5  Convenience wrapper --------------------------------------------------
def chat(message: str) -> str:
    """Send a user message through the manager and return its reply."""
    ensure_fresh_inventory()             
    message = _replace_ordinals_and_pronouns(message) 
    return manager_agent.invoke({"input": message})["output"]